# YouTube Video Transcript

## Metadata

- **Title:** Unknown Title
- **Author:** Unknown Author
- **Channel URL:** N/A
- **Video URL:** https://www.youtube.com/watch?v=UgSGtBZnwEo
- **Duration:** 31:27
- **Views:** N/A
- **Published:** N/A
- **Word Count:** 6024
- **Transcript Generated:** 2025-03-30 23:48:44

## Transcript


### Minute 0

[00:00] it started with open AI 01 next came
[00:04] Gemini 2.0 flash thinking and now you
[00:07] have the Deep seek R1 series The R1 600
[00:13] billion parameter model through the Deep
[00:15] seek API makes it nearly impossible to
[00:18] justify using 01 especially with its
[00:21] virtually Limitless rate limits the R1
[00:24] series lets us scale our compute usage
[00:27] and in the generative AI age compute is
[00:30] how we scale our impact not only do
[00:33] these reasoning models give you wellth
[00:35] thought out answers they also give you
[00:38] the Chain of Thought used to derive
[00:41] their answer the journey your machine
[00:43] takes to arrive at its answer can be
[00:46] just as important as the answer itself
[00:49] as you'll see in this video the internal
[00:51] monologue of these models gives you
[00:53] another feedback loop in addition to the
[00:56] response you can use to improve your
[00:58] prompts at scale let's play with every

### Minute 1

[01:01] one of these powerful reasoning models
[01:04] side by side so you can understand how
[01:06] you can scale your compute when you
[01:08] scale your compute usage you scale your
[01:14] impact let's open up Beni we're going to
[01:17] use the thought bench tool today this is
[01:20] a new tool that lets you compare models
[01:23] and their Chain of Thought side by side
[01:26] we'll start out with a simple prompt
[01:28] ping so so every model is immediately
[01:31] trying to figure out how they should
[01:33] respond to the single word prompt ping
[01:37] if we open up the settings here and
[01:38] shrink the column wi you can see we have
[01:41] all of our model responses and we have
[01:44] their thoughts so let's go ahead and
[01:46] just focus on the thoughts alone so
[01:49] we're working with several deep seek R1
[01:51] models we're also looking at the latest
[01:54] Gemini 2.0 flash experimental thinking
[01:56] model and we're looking at the
[01:57] state-of-the-art reasoning model
[01:59] unfortunately with no thoughts although

### Minute 2

[02:01] the Deep seek reasoning model has made
[02:03] huge strides it does not compare to 01
[02:06] and you'll see additional evidence of
[02:08] that in this video so the fascinating
[02:10] part about these reasoning models is
[02:12] that they have this thought output if we
[02:13] increase the column height here we can
[02:15] see these models thinking through how
[02:18] they should respond we can add both the
[02:21] thought and the response back and you
[02:22] can see we get several variant and two
[02:24] of these the Deep seek and the Gemini
[02:26] flash thinking help us understand how
[02:29] the model is giving us the answer you
[02:31] can see here Gemini flash thinking is
[02:34] really working through all the possible
[02:36] scenarios and what we mean by ping if we
[02:39] scroll down to the bottom here you can
[02:40] see it eventually just says pong and
[02:43] this is what we're looking for right
[02:44] when we type in ping we're just looking
[02:45] for a simple pong response this is also
[02:48] a network protocol command from the
[02:50] terminal and this is where deep seek
[02:53] reasoning decided to go with this right
[02:54] you can see open A1 understood this
[02:57] command better than any model it just
[02:59] responds with pong when you say ping all

### Minute 3

[03:02] you want to get back is pong you can see
[03:04] 8 billion parameter and 1.5 billion
[03:06] parameter giving us a response in a
[03:08] different language so you can see here
[03:09] even with a single word ping having the
[03:12] thought process available is helpful for
[03:14] helping us understand how each model is
[03:17] deriving its final
[03:21] answer so if we reset here with this
[03:24] benchmarking tool we can do a couple
[03:25] cool things here we can add any AMA
[03:27] model we have installed we can also add
[03:29] any one of the common provider models
[03:31] that we want so I'm going to go ahead
[03:32] and add deep seek R14 billion parameters
[03:35] so you can see that got added there and
[03:37] let's go ahead and add a anthropic CLA
[03:41] 35 Hau so let's go ahead and get the
[03:43] latest Hau model just to have this in
[03:45] here for fun now let's go ahead and run
[03:47] an AI coding prompt and let's see how
[03:48] the Chain of Thought can be useful for
[03:50] improving our small AI coding prompt so
[03:52] I'm going to just type create def
[03:55] convert csvs to duck DB CSV paths and

### Minute 4

[04:00] then DB path string as well and this is
[04:03] going to give us back nothing just with
[04:05] this function definition there is enough
[04:07] information here for our reasoning
[04:09] models to fill out the function let's go
[04:11] ahead and fire this off and let's see
[04:12] how our models respond every olama model
[04:15] you see is running locally on my M4 Max
[04:19] MacBook Pro this is a 128 GB unified
[04:22] memory machine this is the topof the
[04:24] line M4 Max so we are blazing through
[04:27] the 1.5 and we're also blazing through
[04:29] the the 8B the M4 can also run the 14
[04:32] the 32 and the 70 billion parameter
[04:34] models so let's go ahead and adjust our
[04:36] width so we can see all of our models
[04:38] here side by side let's see who's still
[04:39] running so we have the Deep seek uh 600
[04:42] billion parameter model this is running
[04:43] in the cloud hence the logo and then we
[04:45] have the Deep seek 14 billion parameter
[04:48] model on my machine you can hear my
[04:49] mforce fan kicking up here language
[04:52] models is the only time I hear these
[04:54] fans kick on if we knock down our
[04:57] displays to only looking at the response

### Minute 5

[05:00] we can see what our models gave us here
[05:02] and so let me go a and shrink down a
[05:03] little bit more so we get Claude 3.5 H C
[05:06] something interesting we can do with
[05:07] this Benchmark is just compare a couple
[05:10] of models side by side so let's say I
[05:12] just want to look at Deep seek 8 billion
[05:14] parameter um let's get the Reasoner and
[05:17] let's go ahead and take a look at uh
[05:20] we'll use 01 as our control model right
[05:22] so we can go ahead and expand the widths
[05:24] here and let's take a look at these
[05:26] answers you can see the 8 billion
[05:27] parameter putting out a decent response
[05:29] here quite a bit of code we are using
[05:31] rm-rf it seems quite dangerous but
[05:35] that's what we have there if we scroll
[05:36] down here nice version from the deep
[05:39] seek Reasoner model and of course we
[05:41] have 01 giving us a great response here
[05:44] as well if we want to we can copy the
[05:46] outputs here and open up a
[05:49] editor and we can see exactly what this
[05:51] looks like right so if we just remove
[05:53] these pieces and take a look at the code
[05:56] you can see here we have a nice
[05:57] functioning result uh just just based on

### Minute 6

[06:00] the function definition we gave right
[06:02] this is a common AI coding technique you
[06:04] can use to generate entire functions
[06:06] just by giving your llm the right
[06:07] information it needs to get the job done
[06:09] we can take a look at Deep seek
[06:11] reasonings response and we're going to
[06:12] get something very similar let's go
[06:14] ahead and drop everything here remove
[06:16] the explanation right very similar
[06:18] result that looks really good you can
[06:20] see that these models are picking up on
[06:22] the duck DB read CSV Auto method this is
[06:26] a really important command that is
[06:28] embedded inside of duct DB that you can
[06:31] use to automatically generate tables
[06:33] from CSV files so it's nice to see that
[06:35] our models are using that and we can see
[06:38] all the way at the 8B size if we copy
[06:41] this out and take a look at this
[06:43] response we do have that kind of scary
[06:45] rm-rf at the bottom here but we can see
[06:48] what that's all about um we have this
[06:50] duct DB SQL not sure uh what that's for
[06:54] but that's there um and so it looks like
[06:56] this is probably going to be a a bad
[06:58] answer it's making up some things and we

### Minute 7

[07:01] can dive into maybe why that is by
[07:04] looking at um of course the thoughts so
[07:07] if we look at just the thoughts of these
[07:08] models let's go ahead and pull in our 14
[07:11] billion as well and let's shrink the
[07:13] column size just a little bit here pull
[07:15] in our 14 billion parameter model we can
[07:17] see something really cool and let's
[07:18] actually just go ahead and reset here
[07:19] and just get our R series models pulled
[07:23] into our view here right so we can see
[07:24] something really interesting right they
[07:26] all kind of follow a similar pattern and
[07:28] that makes sense because because every
[07:29] one of these models was distilled from
[07:32] the deep seek Reasoner model right so
[07:35] you can see the similar pattern if we
[07:36] search for okay comma pretty much
[07:39] everything starts out with okay I need
[07:41] so you can see this kind of similar
[07:43] pattern throughout all the distills and
[07:45] then we have a couple of interesting
[07:46] patterns that we see throughout the
[07:48] thought process so we see you know first
[07:50] comma we see lots of weight comma and
[07:54] you know the weight pops up quite a bit
[07:56] it's really interesting this is how the
[07:57] model kind of double checks itself as if

### Minute 8

[08:00] you or I were thinking and solving a
[08:02] problem but we can see here you know
[08:04] just for this method there's quite a lot
[08:07] of thought going on right and to me this
[08:10] is telling me a couple things right you
[08:11] can see 14 billion parameter look at how
[08:13] much time it spends thinking right we
[08:15] can copy this out paste it in an editor
[08:18] and you know it generated 4,000 tokens
[08:21] of thoughts quite a bit for a relatively
[08:25] simple problem if we copy the thoughts
[08:27] from Deep seek Reasoner we can see
[08:29] something similar so we have 4K from R1
[08:33] 14 billion parameters if we look at the
[08:35] 600 billion parameter model we have you
[08:37] know about 2k tokens so you would assume
[08:40] that you know a larger more powerful
[08:42] Reasoner should be able to solve
[08:44] problems with fewer thought tokens but
[08:46] you can see it working through this
[08:48] something important I want to call out
[08:49] here even the top-of-the-line deep seek
[08:52] R1 model you know 600 billion parameters
[08:55] for a small prompt like this right or a
[08:57] small AI cing prompt it has to do a lot
[08:59] of thinking in order to get this done

### Minute 9

[09:01] right this is a new signal that we can
[09:03] take and say hey let's help out these
[09:05] models right let's help them perform
[09:08] better by analyzing their massive
[09:10] thought process and let's simplify some
[09:12] of that for them right we can do
[09:14] something like this right let's let's
[09:15] continue on the trend of AI coding and
[09:18] let's go ahead and use a AI coding
[09:20] prompt specifically inside of a tool so
[09:23] this is not something that you would use
[09:24] inside of Aer or cursor or something
[09:26] that already exists uh since they're
[09:28] running their own AI Cod prompt formats
[09:30] this is just something you would use in
[09:31] a separate tool so I'm going to paste in
[09:33] this prompt and we can quickly just take
[09:35] a look at this there's nothing you know
[09:37] super special about this aside from the
[09:38] clean format so we're just saying you
[09:40] know generate a function for the given
[09:42] user function request we have a nice
[09:44] Dynamic variable that we're going to
[09:45] update as if this were you know an
[09:47] application and this is getting updated
[09:49] live over and over right so we can go
[09:51] ahead just paste this in here and then
[09:52] we can run that exact same a coding
[09:54] prompt right so we'll say create def
[09:57] string dnot so let's go ahead I'm going

### Minute 10

[10:00] to drop our 1.5b so we're giving our
[10:04] model a bit more information right a bit
[10:06] more help on how exactly to solve this
[10:10] problem we have more instructions we
[10:11] have more details it should be easier
[10:13] for the models to think through this and
[10:15] solve the
[10:18] problem kick this off again let's go
[10:20] ahead and reset so you can see all of
[10:22] our models side by side we can see
[10:23] Gemini flash thinking already completed
[10:26] let's go ahead and shrink the column
[10:28] sizes here so we can see all of our
[10:29] models oh let's get out of thoughts only
[10:31] let's see both sections there we go so
[10:33] we can see a Claude Haiku has a response
[10:35] for us of course it has no thoughts um
[10:38] same with 01 unfortunately this Beast of
[10:40] a model does not give us insight into
[10:43] its internal monologue but we can see
[10:44] here Gemini 2 does have internal
[10:46] monologue we just got our r114 B
[10:49] completed 8B completed now we're waiting
[10:52] on deep seek R1 and the Deep seek
[10:55] Reasoner is hitting the Deep seek API
[10:57] this is the 600 billion parameter model
[10:59] so let's go ahead and start with Gemini

### Minute 11

[11:01] so let's see the thoughts behind
[11:03] Gemini's new output and let's see what
[11:05] this looks like so I'll just go to text
[11:06] file paste this in and look at that so
[11:09] quite a few fewer tokens check this out
[11:11] right only 200 tokens now let's copy
[11:15] deep seek 14 billion
[11:17] parameter okay so deep seek 14 billion
[11:20] look at this 500 tokens okay so by
[11:24] giving our prompt more structure by
[11:27] giving our reing models a lot more
[11:28] information to work with right we have a
[11:30] clear purpose instructions and then a
[11:33] clear request it doesn't need to think
[11:35] so much right and if you're doing less
[11:37] thinking your answer to whatever you're
[11:39] trying to solve is likely more accurate
[11:41] more precise more performant right very
[11:43] cool to see that all the way down to our
[11:46] small model so let's look at our r18
[11:49] billion parameter model here paste this
[11:51] in and you can see here 600 tokens very
[11:54] concise it's working with you know the
[11:56] duck DB functionality it's figuring out

### Minute 12

[12:00] how to uh play with csvs as well we can
[12:03] of course look at the Deep seek Reasoner
[12:06] so this is our you know large model and
[12:08] you can see here something really
[12:09] interesting deep seek 600 billion
[12:11] parameters is actually putting out 2K
[12:13] tokens so still quite a bit but down a
[12:15] lot from the 5K number right and you can
[12:17] see here it's picked up on that key
[12:19] method read CSV Auto so it does see that
[12:21] and it does know to use this method
[12:23] that's really important we going to push
[12:25] to looking at just the response and one
[12:27] of our keys here is that we only want to
[12:30] see the code okay so where do we see
[12:33] that here yeah do not include any other
[12:35] texts do not include any other code so
[12:37] we just want the output of this method
[12:39] right and let's go ahead and dial into
[12:40] just a couple uh answers right let's
[12:42] look at R1 let's look at R1 Reasoner and
[12:45] let's look at 14b close this up a little
[12:47] bit here expand the column width I'm
[12:49] really enjoying using this tool um Link
[12:51] in the description by the way for this
[12:53] we're building on Beni this is a suite
[12:55] of benchmarks that you can feel that I'm
[12:57] building on the channel as I'm working

### Minute 13

[13:00] through analyzing and building with
[13:03] large language models I want to share
[13:04] some of my tooling with you that's what
[13:06] this is feel free to get this link in
[13:08] the description a tool like this is
[13:10] super important not only for the
[13:11] thoughts but just to compare models side
[13:13] by side with different prompts that you
[13:15] might have with different ideas you can
[13:17] easily come in here and just add
[13:19] arbitrary models um as long as you get
[13:21] the model name right and use one of the
[13:23] um available model prefixes you can use
[13:26] Gemini colon open AI colon any olama
[13:29] model and anthropic Link in the
[13:32] description for you if you're interested
[13:33] a lot of the work I'm doing behind the
[13:35] scenes I can't always share but when I
[13:36] can i' love to share it with you here on
[13:39] the channel let's look at these
[13:40] responses so we're looking for just this
[13:42] concise code if we copy deeps Reasoner
[13:45] you can see we're getting a great
[13:47] response out of its thinking tokens here
[13:49] it thought all of this and then it
[13:51] outputed just this if we go language
[13:53] mode markdown you can see how concise
[13:56] this is right this is a near perfect
[13:57] answer create replace table name and

### Minute 14

[14:00] then select from read CSV Auto and then
[14:03] it's escaping this that looks great what
[14:05] we get out of this is a duck database
[14:08] with all of these tables created from
[14:10] these CSV paths right we can test this
[14:13] of course against 01 and we can see 0
[14:15] one's response as well it's going to
[14:17] look very similar because these are
[14:19] basically you know the perfect answers
[14:21] here let's go language mode python same
[14:25] deal right so you can see here Reasoner
[14:28] a little bit more ACC by pulling the
[14:30] Imports out of the function and so very
[14:33] interestingly here it looks like 01 may
[14:36] have made a mistake we are selecting and
[14:38] importing everything oh it's just
[14:40] creating the table here in the first
[14:42] statement you can see limit zero and
[14:44] then it's inserting from using read CSV
[14:47] Auto very interesting so anyway so we
[14:49] can see here with the small model right
[14:51] 8B having trouble following the
[14:53] instructions right we don't want
[14:54] anything else we're just looking for the
[14:57] answer but still not too bad you can see
[14:59] it got uh in answer it's not going to be

### Minute 15

[15:02] perfect here it's actually definitely
[15:04] wrong but that's fine um it is a small
[15:07] compact model so let's go ahead and look
[15:09] at the answer from our 14 billion
[15:11] parameter model here and we'll just copy
[15:14] that there and let's see how we've done
[15:16] here so a little bit more Rose we can
[15:19] see we have the table name there that's
[15:20] good started playing with the schema
[15:23] then it dropped it we have execute many
[15:25] we don't need all this so it's it's not
[15:27] performing very well right at some point
[15:29] the smaller you go the less these models
[15:31] are going to be able to do uh and we can
[15:33] see that here so how is this stuff
[15:35] useful having both the thoughts and the
[15:37] responses of these models gives you more
[15:40] information to help you improve your
[15:42] prompt even further right it's also
[15:44] helpful to just see where the limits are
[15:47] right we can see that for the 14 billion
[15:49] and the 8 billion parameter deep seek
[15:51] models they simply cannot accomplish
[15:53] this task without additional information
[15:56] so for instance we can give them another
[15:57] shot by improving the prompt a little

### Minute 16

[16:00] bit more let's try and guide our 8B and
[16:02] 14b what we'll do is we'll copy we'll
[16:06] reset we'll paste The Prompt get rid of
[16:09] 1.5b we'll keep 8B and what we'll do is
[16:12] We'll add 14 and then we'll also add 32
[16:17] let's add a couple additional details
[16:18] here to help our model get the response
[16:21] we're looking for we can use the
[16:23] thoughts if available and the response
[16:25] from our large models like deep seek
[16:28] Reasoner open AI 01 and Gemini's 2.0
[16:31] flash thinking to guide the smaller
[16:34] models and more importantly to guide our
[16:36] prompt to more precise output so we can
[16:39] say something like this right use duck
[16:41] DB's SQL star
[16:44] from read CSV and I think we want read
[16:48] CSV Auto right yeah read CSV Auto read
[16:51] CSV Auto just input. CSV okay and then
[16:54] we saw the small models we making a
[16:56] couple mistakes here so I'll also say
[16:57] use the CSV name as the table

### Minute 17

[17:00] name we're going to run deep seek r18
[17:03] billion parameter 14 and 32 that's
[17:06] running on my M4 right on my device
[17:09] we're running deeps gaser in the cloud
[17:11] 01 in the cloud and Flash thinking in
[17:13] the cloud you can see here 01 came back
[17:16] with a great response we can go ahead
[17:18] and just dial into our top performers
[17:19] here right our Mega Cloud models uh you
[17:22] can see them spinning out just a great
[17:24] answer we can see again this trend
[17:26] continuing of Gemini the the more
[17:29] precise your prompt is the less the
[17:31] model has to guess right and we can see
[17:33] that we can confirm that through the
[17:35] chain of thought so we can see Gemini
[17:37] just thinking through everything it's
[17:39] looking at the parameters it's figuring
[17:40] out what that means and how to form the
[17:42] right response so you can see it's doing
[17:44] some nice table name manipulation off
[17:46] the CSV paths it's looping through those
[17:49] we can see a similar output here from 01
[17:51] and let's take a look at our small
[17:53] models so let's look at 8B 14b 32b side
[17:57] by side we'll drop the Comm width so we
[17:59] can fit them all here on screen and

### Minute 18

[18:01] let's take a look right so we can see 32
[18:03] billion parameter is giving us a nice
[18:06] concise response let's copy this out and
[18:09] I actually like this a little bit more
[18:11] than you know this is uh open AI
[18:13] response and this is going to be uh deep
[18:16] seek R1 32b check this out right we have
[18:20] the connection We're looping through
[18:22] we're using the exact same code let's go
[18:24] ahead and convert this to python we're
[18:26] using the exact same code drop this down
[18:28] down here that's going to be much better
[18:30] to compare so if I go down here there we
[18:32] go deeps on the bottom open AI on the
[18:34] top While We're looping through this we
[18:35] can actually see like the perfect
[18:37] response coming out of this although you
[18:39] know now that I'm looking at this side
[18:41] by side once again you can see open AI
[18:43] 01 is a little bit ahead right you have
[18:45] to create or replace this I don't know
[18:47] if select into Works without the table
[18:50] existing first right I need to actually
[18:52] run this command to C so R1 32b getting
[18:56] closer here right and we were able to
[18:58] get these models a lot closer by looking

### Minute 19

[19:01] at both the thoughts and the output we
[19:03] can see 8B here um still having some
[19:06] trouble it is generating the code that
[19:08] looks a little bit better but we can see
[19:10] here in the 14 billion parameter we're
[19:13] still getting a little bit more text
[19:14] which we're not happy about uh but we
[19:16] are getting a more precise answer okay
[19:20] so this is great to see and we can dial
[19:22] into the big hitters deep seek Reasoner
[19:24] 01 and thinking and you know we can see
[19:28] you know the kind of kind of you know
[19:29] perfect responses here deep seek with
[19:32] the create table 01 create or replace
[19:36] and then uh thinking with the create
[19:38] table so what's happening here why is
[19:39] this important it's important because
[19:41] we're acting at the intersection between
[19:43] two feedback loops now we have both the
[19:45] response from our powerful high-end
[19:48] Cloud reasoning models and minus 01 we
[19:52] have the actual thoughts that they use
[19:55] to help derive the answer and by looking
[19:57] at the thoughts we can use that
[19:58] information and the response to act as

### Minute 20

[20:01] inputs to our personal feedback loops
[20:04] that can improve our prompt and I hope
[20:06] you can kind of see you know where that
[20:08] could take us here on the channel every
[20:10] detail of your prompt matters literally
[20:13] every single character can change the
[20:15] outcome and so the thoughts in
[20:17] combination with the response in
[20:19] combination with looking at models side
[20:21] by side like this gives us more
[20:23] information to improve the prompt so
[20:25] that ultimately we can at some point
[20:28] hand off the process of generating
[20:31] prompts to an AI agent we've talked
[20:34] about this pattern a little bit in our
[20:36] previous meta prompting video I'll also
[20:38] link that in the description and
[20:40] speaking of that meta prompt let's go
[20:41] ahead and use that as another example to
[20:43] see how many of these models how many of
[20:45] these powerful local R1 thinking models
[20:48] can keep up with a very very complex
[20:51] prompt that is the meta
[20:55] promp so we'll hit reset here this will
[20:58] get us back to a base state I'm going to

### Minute 21

[21:01] go ahead right away and drop off 1.5b
[21:03] and 8B they will not be able to run the
[21:06] meta prompt and just to kind of show you
[21:09] what the meta prompt looks like I'll go
[21:10] ahead and clear I'll paste this in here
[21:14] let's change our language mode to XML
[21:16] and The Meta prompt itself is a 2,000
[21:20] token prompt okay so this is a
[21:22] non-trivial large prompt so this prompt
[21:25] is special because it generates other
[21:27] prompts okay
[21:29] so what prompt do we want to generate
[21:30] with our meta prompt let's keep it
[21:31] relatively simple I'll say purpose
[21:34] convert the given text into a markdown
[21:37] table instructions and then we see here
[21:39] we have some nice uh cursor tab
[21:42] completion coming in our examples we
[21:43] don't need that for a meta prompt we
[21:45] want user input we want text blob and we
[21:47] want uh table columns right so you can
[21:50] imagine this prompt is going to have
[21:52] these three additional XML blocks at the
[21:54] bottom where we can fill in Dynamic
[21:56] variables and then we have our
[21:58] instructions here um user input text

### Minute 22

[22:01] blob and table columns
[22:04] create a markdown table great and I'll
[22:07] say cover every column detailed I'll
[22:10] also say and we can new onine this just
[22:12] to clean it up a little bit use mark
[22:14] down table syntax any other text include
[22:19] a table header H1 and a table footer H2
[22:24] with bullets one for each column
[22:29] explaining uh the column great okay so
[22:32] this is the meta prompt I'm going to
[22:33] copy all this we're going to paste it in
[22:35] our thought bench let's add a local
[22:37] model so let's go ahead and run 32b
[22:39] let's see if 32b can keep up colon 32b
[22:42] we'll hit add and let's go ahead fire
[22:45] off these four models and let's see
[22:47] let's also do a 14b I'm just curious if
[22:50] 14b can pull this off I doubt it but
[22:52] let's go ahead and add 14b and let's go
[22:54] ahead and Fire Off The Meta prompt so
[22:55] this prompt will generate a prompt that
[22:57] does this for for us right we will hit

### Minute 23

[23:00] thought prompt I mentioned I'm running
[23:02] my M4 Max and when I'm running 14b plus
[23:06] size models that's the only time I ever
[23:09] hear my M4 actually turn on so you can
[23:12] see we're getting some responses coming
[23:14] back in here let's go ahead and close
[23:15] the width a little bit so we can see all
[23:17] of our models you see how long this is
[23:18] for Gemini flash it's thinking quite a
[23:21] bit and we can see deep seek Reasoner
[23:23] also you know quite a bit of thinking
[23:25] tokens here we can copy it and take a
[23:27] look at what it thinks about the meta
[23:29] prompt so uh it's looking at the purpose
[23:32] instructions sections right it's
[23:34] breaking down the meta prompt itself and
[23:37] so really cool it sees that variables
[23:39] are correctly placed with this syntax so
[23:42] it's using two squares this here is an
[23:45] instruction so it is reading the
[23:46] instructions properly and let's see what
[23:48] our model actually output here so this
[23:50] is deep seek R1 and if we just paste its
[23:53] response perfect right this is perfect
[23:56] this meta prompt generator a new prompt
[23:59] for us you can you can see here it even

### Minute 24

[24:01] has that uh it's using that leading text
[24:03] prompt engineering technique that helps
[24:05] the llm start its response it has our
[24:08] three Dynamic variables that we asked
[24:10] for user input text blob table columns
[24:14] it's got clear instructions and you can
[24:16] see here it's got a nice purpose you're
[24:17] an expert at transforming unstructured
[24:19] text into well-formed markdown tables
[24:21] fantastic right if we look at our 32b
[24:25] and our 14b they just can't do it right
[24:29] look at these responses let's go ahead
[24:30] and hone in here and look at these
[24:32] responses right somehow throughout this
[24:35] process right The Meta prompt in itself
[24:38] contains three other examples of running
[24:41] the meta prompt right so it's very
[24:42] confusing for a language model you need
[24:44] a lot of size to really understand this
[24:47] prompt so you can see our responses are
[24:49] just completely bogus here they're
[24:50] outputting like a garbage mock table
[24:53] these are just immediately you know
[24:54] unusable for this use case um but we can
[24:57] see here deeps Reasoner
[24:58] 01 and Gemini thinking if we compress

### Minute 25

[25:02] these a little bit these top tier Cloud
[25:04] models all likely above you know 300 400
[25:07] 500 billion parameters these can all do
[25:10] the job well let's copy out 0 one's meta
[25:12] prompt so this is deep seek R1 this is
[25:17] o1 you can see here 01 is giving us
[25:19] these like table columns um and then we
[25:22] have let's pull in Flash here not too
[25:26] bad but here we have us user prompt and
[25:28] user input we don't need both of these
[25:30] we need one or the other so interesting
[25:33] to see that here flash is a little bit
[25:35] behind 01 and deep seek Reasoner this is
[25:38] one of the most complex things you can
[25:40] do with these language models ask it to
[25:42] be you know kind of self-aware ask it to
[25:43] do meta level thinking this is where a
[25:46] lot of the value is right now in
[25:48] generative AI in prompt engineering is
[25:50] having these models you know improve
[25:52] other models um we can see we're getting
[25:54] some decent responses out of this right
[25:56] and I want to mention once again we we
[25:58] can come in here look at the thought

### Minute 26

[26:00] process let's go ahead and expand this
[26:01] height a little bit and let's just look
[26:04] at our reasoners that give us the actual
[26:07] output so let's look at these two and
[26:09] let's pull in our 32 billion as well
[26:11] right right so let's say we want to
[26:12] improve the meta prompt or simplified or
[26:14] something right we can go through and
[26:16] make improvements by looking at the
[26:19] thoughts of these models side by side
[26:21] especially when we have all three of
[26:23] these okay and so you can see
[26:25] here uh 32 billion parameter just go
[26:28] goes off the walls quite quickly here um
[26:31] it thinks that its job is to actually
[26:33] execute that prompt so anyway so last
[26:36] thing we'll do here just for fun let's
[26:37] go ahead and take the prompt let's take
[26:39] the generated prompt from Deep seek r1's
[26:43] meta prompt let's just copy this it's
[26:44] going to be fun let's clear let's we'll
[26:47] do a full reset we'll paste this in and
[26:49] we're actually just going to run our
[26:50] generated prompt from our meta prompt so
[26:53] um I want to say generate model
[26:55] comparison table given the text blob
[26:59] uh header is model price okay and then

### Minute 27

[27:02] we can specify the column so I want
[27:04] model Alias input tokens uput tokens
[27:08] input costs uput cost okay and you can
[27:11] imagine what I'm going to paste in here
[27:13] I'm going to go over to deep seeks model
[27:16] pricing just going to copy all this and
[27:18] just as a blob we're just going to paste
[27:20] this in right so we have that there and
[27:22] then I'm going to go over to 0 one's
[27:24] model pricing page just going to copy
[27:26] the O Series model
[27:28] just going to paste this in here and
[27:30] then I'm also going to grab the 01 And1
[27:32] mini just copy their input and output
[27:35] and just paste that in there right and
[27:37] then um I'm going to also update the
[27:39] columns a little bit I'm going to say
[27:40] Max input tokens and Max output tokens
[27:43] and let's go ahead and see how models
[27:44] perform here once again I'm going to
[27:45] drop 1.5 I'll keep 8B and I'll add in um
[27:51] let's see how 32b does here okay and
[27:55] just for fun we got to show love to the
[27:57] OG I'm going to throw in anthropic CLA

### Minute 28

[28:01] 35 Sonet latest let's go Ahad and firey
[28:04] this off right let's see how our models
[28:06] perform here let's drop them all down so
[28:09] we see this side by side and we can see
[28:11] of course we don't have thoughts for
[28:12] anthropic but if we copy out um sonnet's
[28:16] response here so if we open up a new
[28:18] file here and paste this in format the
[28:20] results we can see from Sonet we can get
[28:23] a decent breakdown of the model Alias
[28:26] the max input Max output input costs per
[28:29] million and our output costs per million
[28:31] you can hear my M4 working hard to get
[28:33] the 8B and the 32 there's the 32 let's
[28:36] copy the 32 billion parameter model
[28:38] let's see how it's performed here so
[28:39] this also looks good 32 billion
[28:41] parameter model actually doing some good
[28:42] work for us here giving us a nice model
[28:45] comparison and this can go on down the
[28:48] line right we don't need to check 01 we
[28:49] know that'll be great we can quickly
[28:51] look at uh deep seek do a quick model
[28:53] preview here you know deep seek looking
[28:56] great for us here you can see here we
[28:58] have the model aasis set up properly uh

### Minute 29

[29:00] we're missing 01 here that's fine these
[29:02] are all Minor Details but you can see
[29:04] this working and you can see this being
[29:06] useful right so if we just expand
[29:07] everything we can get that and if we
[29:09] want to we can just focus in on the
[29:11] responses alone this tool is going to be
[29:13] linked in the description if you want to
[29:14] check this out this is called thought
[29:16] bench and it's a way to look at models
[29:18] side by side and iterate on prompts
[29:21] across many models in a live
[29:24] benchmarking way with new generation
[29:26] reasoning models like R1 and Gemini
[29:29] flash thinking we can peer into the
[29:31] thought process of our machines this
[29:33] enables us to gain insight into how we
[29:36] can improve our prompts to drive improve
[29:39] results across execution of our language
[29:42] models at scale it's important to call
[29:44] out that the Deep seek R1 series
[29:47] represents a massive continuation and
[29:49] really acceleration of the trend we've
[29:52] been betting on and predicting on the
[29:53] channel price is going down reasoning
[29:56] abilities are going up up and speed is
[29:59] going to go up basically we're getting

### Minute 30

[30:01] massive amounts of compute every single
[30:03] month now and it's up to us to figure
[30:05] out how to best use it and how to best
[30:08] ingest the capabilities of these models
[30:10] this is why I built Beni by the way it's
[30:12] up to us to figure out how we can best
[30:14] use this compute and understand these
[30:16] models so that we can deploy them at
[30:18] scale like I said in the introduction if
[30:21] you want to scale your impact in the
[30:23] generative AI AG you need to be scaling
[30:25] your compute there's going to be a one
[30:28] one to one linear line between your
[30:31] impact your output and how much compute
[30:34] you're using the correlation is going to
[30:36] be a causation very very soon the models
[30:39] are improving but as we saw here even 14
[30:42] and 32 are still lacking I didn't test
[30:45] 70 billion here for time sake but that
[30:47] model is a step change above 32 so I
[30:50] highly recommend you check that out but
[30:52] the very clear winner here is R1 the
[30:55] price for this is insane it makes using
[30:57] o one basically impossible just because

### Minute 31

[31:00] you're getting so much more value about
[31:03] 25x value with only a slight loss in
[31:07] capabilities so let me know in the
[31:10] comment section how are you liking the
[31:12] R1 series are you getting value out of
[31:15] the Chain of Thought of these models if
[31:17] you enjoyed this video you know exactly
[31:19] what to do drop the like comment and sub
[31:22] stay focused and keep building