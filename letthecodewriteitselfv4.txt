Welcome to Principled AI Coding Lesson 7.
In this lesson, we accelerate on the parabolic curve of AI coding.
This is the lesson where your abilities can multiply far into the sky.
In this lesson, we travel into the dark hole and peer into the future of AI coding.
I'll say this up front.
This will be the most controversial lesson of the course.
Because here we can start to see the edge of software engineering.
And it's unclear what's beyond the edge.
Let's dive right into the principle for this lesson.
In this lesson, we close the loop and we let the code write itself.
What does that mean and how is it possible?
In this lesson, we push into the implication of AIDR's secret.
The fact that AIDR is programmable and scriptable means that you can build
agentic workflows that solve coding problems for you.
Once again, the only question is, can you align the big three context model and prompt?
To give your AI coding assistant what it needs to accomplish your tasks.
The big three bullseye, just like our other principles,
remains just as important if not more important as you scale up your AI coding abilities.
Every lesson, every principle builds on each other.
And they become more important, the more you're pushing your AI coding abilities.
In this lesson, we add the idea of closing the loop as the next principle
on top of everything you've learned so far.
So what does this principle look like in practice?
Let me introduce you to the director pattern, also known as the director loop.
This is an agentic workflow where you define your context, prompt and model.
As usual, and then you define an execution command and an evaluator.
These are the two missing pieces you need to close the loop.
Your execution command runs after your AI coding assistant has generated your code
and then your evaluator takes the output of your execution command and determines
if your task is done, if your task has not completed, it creates feedback
and then loops back in to your AI coding assistant closing the loop.
An evaluator can be a piece of code, but we're going to focus on evaluators
that are themselves large language model calls.
This is a pattern from language model testing.
You may have heard called LLM as a judge where you have an LLM grade the output
of another LLM with this pattern.
We close the loop because now you can write a prompt, have your AI coding assistant
generate code, execute the code, evaluate the code and if the evaluation fails,
pass the feedback back into the AI coding assistant.
This pattern is the prototype to the future of AI coding.
The future where we let the code write itself by building upon patterns
we've established and the AI developer workflow and the spec prompt.
In this lesson, we close the loop by giving our AI coding assistant feedback
through an evaluator function.
With this pattern, even in its early form, we are approaching the edge of AI coding
and potentially the edge of software engineering as we know it.
This pattern enables AI coding tools to act in a more autonomous way,
solving your problem iteratively until it's complete.
Next generation AI coding tools like Devon and GitHub's Copilot workspace
rely on patterns like the director pattern to run autonomously.
Now let's come back to Earth and walk through a concrete example
of how we can close the loop and let the code write itself.
Let's open the principal AI coding directory.
If we type LS, you can see our previous six code bases.
Let's get clone the lesson seven code base.
Link will be in your loot box as always.
I'll CD in to our pack seven code base and open VS code.
Right away, I'm going to drag my dot environment variable file
from my previous code base into the directory.
Now I have access to OpenAI and Anthropics API keys.
Let's open the readme, open a terminal and let's get started.
I recommend you set up both the OpenAI key in your dot environment variable file
and the Anthropic key we're going to be using both throughout this lesson.
Let's run UV sync to install our dependencies.
And just for good measure, we'll run UV run main
to make sure that our application is working perfectly.
We'll be building off the latest version of our transcript analytics application
to understand this new director pattern.
We have our output as expected.
And inside the readme, you'll notice we have a new command here
where UV run python director dot py with this config flag past 10.
Let's go ahead and open up this director configuration file
and break down the director pattern.
So we have a new YAML file format here,
which you'll notice key value pairs you've already seen.
At the top of the file, we have a prompt filled with information dense keywords.
You can see we have update keywords, create keywords, add file references,
function references.
And all we're doing here is building out a brand new green themed HTML base file output.
We update the output format.
We update our main file.
And most importantly, we're updating a test.
So we have our prompt and then we have our model.
You can see here our coder model is defined to be the clawed 3.5 IQ model.
You can update this to be any Adder compatible model you like.
And then we have our context.
We have editable context and read only context.
You can see this is set up much like our ADWs, our AI developer workflows.
And then we have the magic of it all, the execution command and the evaluator.
You can see the execution command is a UV run pie test command.
We're running all of our tests and we're disabling warnings.
So this is going to be really important.
We're going to circle back to this.
We're going to quite literally circle back to this in our code.
Below this, we have our max iterations.
So our ad coding director loop will only run five times at most.
We have our evaluator model set as GPT 40 and then we're running our default evaluator method.
As you build up the director pattern, you'll want to build up different evaluators.
We'll set this as default and explain what's happening here in just a moment.
That's our director configuration file.
Now let's go ahead and open up the director Python file.
You can see this is right on the top level of the code base.
I'll collapse everything to the first level and let's walk through this file.
So at the top, we have our evaluation result.
This is what our evaluator method is going to return.
It's either going to be success, true or false.
And then if false, we're going to have we're going to have a piece of feedback as a string.
Next, we have our director configuration.
This is all the fields we just saw from our director file here.
You can see for our evaluator models,
we can choose any one of the GPT 40 models as well as the O1 reasoning models.
By the time you're taking this course, the O1 model may have fully been released.
So you can add that here if it's available.
And then we have the director class.
I'm going to open this up and then we'll collapse down to level two.
So we'll fold our code to level two.
So we can see the key director methods.
So here we can see at a high level, everything the director does.
We can walk through this process top to bottom.
First, we create a new AI coding prompt.
We then pass the prompt off to Ada.
And Ada will generate the code we're looking for based on our big three.
Then we execute our execution command.
And remember, that's our pie test call here.
So at this point in the code, we're going to execute code.
Right?
We're actually running code after our AI has generated code for us.
We're going to execute that newly generated code.
And then we're going to evaluate the result just by looking at the function stub.
You can see that the evaluator takes the execution output as a parameter.
And then returns the evaluation result, which we saw up here, success and feedback.
Finally, we have the direct method.
And this is how we kick it all off.
Let's open this up and take a look at what this looks like.
So just as described, we have this loop that runs up to five times
based on our max iterations variable.
And if we open this up, we can see the director pattern top to bottom.
First, we generate a prompt, we then run that code.
We then execute our execution command.
We then take the result of our execution and run our evaluate method on it.
And then our evaluation either returns true or false.
If success, we break the loop, the work is done.
If false, we need to run again.
We need to restart the loop and let the code write itself in this loop here.
We continue to the next iteration and the evaluation.
If we just go ahead and search this, you can see the evaluation gets passed in
to our create new AI coding prompt, thus closing the loop.
So before we move forward and dig into any additional details here,
let's just close this and let's fire off this director pattern.
So this is going to generate a green theme HTML output type.
We're going to trigger with HTML G.
And then it's going to write a test for us.
And this is the most important part.
Why?
Because our execution command will then run this test.
And if it fails, it's going to pass the failure back into our AI coding assistant.
Okay, this is a really simple prompt.
There's really nothing to be seen here, but I just want to run this through.
And I wanted to use this as an example to introduce the director pattern to you.
So watch the output here.
So you can see iteration one out of five create new prompt generating echo.
So now Ada is running.
Ada has taken over and we are running the high coup model.
And you can see here, just as before, you know,
nothing news happening here, there's just our AI coding assistant running with our prompt
that we've passed in with the given model and the context.
So we're going to get a green themed HTML output file here.
You can see our main file got updated.
And finally, our output format test was updated.
And now here at the end, you're going to see our test get executed.
Okay, so you can see executing code evaluating results.
And now we've reported success.
So our director loop with our AI coding assistant are execution command
and our evaluation has reported success, right?
So this is really interesting.
Obviously, this is a simple example, right?
We're running three information, dense keyword prompts.
So there's not a lot of room for error here.
As you'll see in the next example, this loop is very, very powerful.
When we give our AI coding assistant all the information it needs
and let it run in a loop with legitimate feedback,
it can solve some pretty heavy problems.
But it's all based on our execution command
and how we have our evaluator method set up.
So as an engineer, it's always good to be skeptical of technology
or tools or patterns.
So you can execute this yourself.
You can copy UV run pi test and literally just run what was just executed.
You can see all of our tests pass.
We can open up output format.
We can see our new green theme format.
That's wonderful.
Now we can go ahead, open up or read me copy this run example.
And instead of running a dot YAML file,
we can update this to a dot HTML G file, right?
Because that's how we described how to generate our new format here, right?
If I just search HTML G globally,
you can see main has a new else if block looking for a dot HTML G.
So this is great.
We can go ahead and kick this off and our test pass.
We know that this structure of code works.
So we can be very confident that this will execute properly.
We can see we got the output transcript.
Let's open that up.
You can see here we have some green colors in our style.
Let's open up HTML preview and see the results.
So wonderful, right?
We have a green themed HTML file format for our transcript analytics application.
Fantastic.
We have some really nice styling here.
We got these nice, you know, side borders.
Really nice iteration here from our a coding assistant.
This shows off the kind of point blank capability here, right?
You can kind of see where this is going.
You can see what's going to happen next when we throw a harder task at the
director pattern at this a genetic workflow, right?
Let's kick things up a notch.
Let's move on to a more difficult example.
Let's open up the slider director slider output file.
So you can see here in our specs file,
we have two director files, one director template that you can use to,
you know, use this pattern and build off the pattern.
And then we have two spec files, right?
Very importantly here, you'll see in the director slide.
Let me close these other files here.
You'll see in this director configuration file.
Instead of a full prompt, instead of writing the prompt here,
we're actually passing a spec prompt.
So let's go ahead and open this up.
And I actually need to update the name of this.
I need to update to spec underscore feature.
Let's go ahead and save that.
Okay, so now that should be this file perfect.
And you'll see what you've seen before here, right?
We covered this in lesson six.
We have a spec prompt that contain this specifications
for our brand new feature that we're building.
So we have the header here.
We have the high level objective.
We're creating a new interactive HTML output format
with a dynamic word frequency visualization.
So we want a slider, which allows us to, you know,
filter in and out our word frequencies.
We're also going to add two new visualizations
for our word frequencies, tell us and just the information.
And then of course, you know, as before,
we have our implementation notes, context,
and most importantly, our low level task,
which contain some high quality prompts using techniques.
We covered in lessons one through four.
So, so just to wrap the configuration file,
we have the prompt, we have the model.
And of course, we have our configuration.
So this gives our coding assistant.
It gives the director pattern.
Everything it needs to execute on this code.
Remember, great planning is great prompting.
We're setting up all of our AI tooling for success.
What we're really doing is embedding how we ourselves,
the engineer would solve this problem, right?
So let's go ahead and keep moving here.
So we have the context and then we have our execution command.
So once again, we're just using pie test.
It's important to call out this could be anything.
So if we wanted to,
this could be running our main transcript analytics application
with this new chart type, right?
So we would say something like this chart type.
And then, you know, we would expect something like a radio.
But what we're doing here instead is
making sure that in our spec prompt,
we're saying you need to create tests for this
because we're going to run those tests
to determine if you were successful.
That's what our execution command is doing here.
Okay, we have our max iterations.
And then here's something really cool.
We're using a 01 reasoning model to determine if we were successful.
And I want to run this first and then we can dig into
what our evaluation method actually looks like.
I really want to focus here on the high level pattern
of the director loop and not the actual details
until further on in this lesson.
The key here is in the pattern.
Before we execute this,
I'm going to roll back previous changes.
We don't need these mucking up the state of our application.
If you open up the AI code block here,
I'm running with auto commits off.
So it or did not commit anything for us.
I'm going to go ahead and roll these back.
I'm going to run git restore and then source.
So everything under source, I want reset.
And then I'm going to just remove our output.html.
That doesn't really matter.
I'm just going to remove it for now.
So everything here looks good.
Let's go ahead and fire off the director pattern.
So I'll open the readme copy this command.
And before we run it, of course,
I'll clear this, get a reference to the file
and run the director slider instead.
Here we go.
So let's let's walk this through
and we're going to see something really, really cool happen here.
We're going to watch our code right itself.
So iteration went out of five.
We're now starting to run through the prompt.
Okay, we're going to get format as HTML with slider filter.
So we should get some JavaScript at some point here
getting generated.
There's a table and there's some script.
So there's our JavaScript slider.
Wonderful.
And now Haiku has stopped.
It didn't finish the job.
It's asking us if we wanted to proceed.
Some models do this.
Our evaluator just ran and it caught that.
It said, we're not done.
The test passed.
However, the new test for the HTML slider
and the new chart visualizations are missing.
So our evaluator, although our tests pass
our evaluator said, no, you're not done based on the prompt
based on all the information passed in.
You're not done.
Run another iteration.
So you can see here iteration two out of five.
And now our AI coding assistant is based on the feedback
and original instructions.
I'll focus on adding conference of tests
and implementing the new chart visualizations and HTML slider.
Here we go.
So now it's taking the feedback and that's continuing.
So create land chart.
There it is, create radial bar chart.
Right, we have our bubble chart there.
Fantastic.
Here's our chart test.
Our chart tests are getting updated here.
Okay.
And then our main file is getting updated.
You can see here we have the new radial and bubble
coming in here.
Fantastic.
We're getting a summarization.
And then look at this.
We have a error here.
So we've got a parsing error for the O1 model.
That's fine.
It's going to fall back on our 4-O model.
This happens sometimes.
And our 4-O model here now is running evaluation
and it's saying almost most tests have passed.
But one critical test has failed.
You can see there's a met plot live error.
We got to understand back to keyword.
I'm actually not even sure what exact is going on here.
But that kind of highlights the point.
I don't really need to know what's going on.
Our evaluator, LLM call, is going to direct
our code assistant to solve the problem, right?
I'll modify to use plot subplots, okay?
So we have that update.
We have this test running.
Everything passed.
However, the test for the HTML output format
with slider was not added.
So again, our evaluator caught something
that was just missed.
You know, Haiku just completely missed adding a test for this.
So it's running the fourth iteration here, okay?
Without us doing anything, right?
I haven't done anything.
And now we can scroll here.
We can see that that new test was added.
Format HTML with slider filter.
And we can see here.
We got a linting issue here.
Aitor caught.
That automatically got fixed.
And here we are.
A four iterations later.
Breaking out of the loop.
Our director pattern.
Our AI agents and the evaluator just had four conversations
about what went wrong and how to improve it, okay?
Let's just take a moment and, you know,
internalize how incredible that is.
Incredible that was, right?
The code is writing itself.
And I know you're thinking of a million caveats and what ifs
and situations where this won't work and, you know,
you saw the O1 mini model bomb there on the parsing.
Yes, yes, yes.
There are many issues with this loop.
There are many ways to improve this.
There's lots to be desired.
But it is the pattern that is important here.
And it's the principle behind the pattern
that is even more important.
We are closing the loop with just
300 lines of code, not compact code,
not the cleanest code, not the most concise code.
But in just 300 lines of code,
we have an agentic AI coding workflow
that runs code based on a great plan
that we have been building up the knowledge and the skill
to know how to build.
And then we're handing off the coding process
to our director pattern, to our AI coding assistant,
to our small AI agent team here.
Okay, so absolutely incredible stuff here.
Before we jump to the details
of the create new AI coding prompt, execute and evaluate,
let's just run this code.
Remember, we are becoming curators of code.
We're becoming curators of workflows
and we're becoming lead engineers,
engineering managers.
We're moving ourselves up the stack
and we're handing off the low level lines of code,
the details, the how.
We're handing that all off to our AI coding assistant
and our director loop more and more and more.
Okay, so let's just go ahead, do a sanity check.
We're just gonna run all of our tests.
You can see everything pass as you use this pattern.
You're just gonna start trusting the system a lot more.
The test ran, that's the evaluation command
that we set up here.
We are using a more powerful O1 reasoning model.
This means that we're going to have a more reliable output
when this executes and when we get our evaluations.
And by the way, while you're running this,
everything is getting logged here
to the director log.txt file.
So if you scroll down here,
you can see what everything looks like exactly.
And so you can see the outputs.
You can see the test execution there passing great.
We can scroll up.
We can see these are all of our editable files
and this kind of smashed JSON format.
You can see our prompt and you know,
you can really dig in if you want to in this log file here.
But so we have this O1 model.
Let's go to the readme and let's go ahead
and execute some of this, right?
Let's make sure that our code did what we wanted to, right?
This was a, you know, let's just not forget here.
This spec prompt is fairly sizable, right?
We have a decent size context.
Not a large context.
Five files is not large by any means,
but you know, medium sized.
And we have a quite a few requests here.
And you saw Haiku kind of doing one thing at a time,
taking breaks asking if you wanted to do more.
Other models may or may not do that.
They might not stop.
Other models will definitely stop
and do a more iterative task flow.
That doesn't matter.
Our director pattern took care of that for us.
At the bottom here, we have our four prompts,
or four A coding prompts.
And these are not trivial by any means.
In test four here,
we're having it in for a lot of information, right?
Add these tests.
These are, you know,
mid to high level prompts here
in our, especially in this last prompt, right?
But let's go ahead and execute this
because we want to know that it's working.
We are still validators
and we are still kind of, you know,
managers of the output
of our A coding assistance.
So let's go ahead and,
let's go ahead and try the slider first.
So I think we're doing HTML SLD.
Did I read that right?
SLD, there we go.
Yeah, so our new extension output
to trigger this file format is HTML SLD,
which stands for slider.
Let's go ahead and kick this off.
And let's see the output of this.
So that executed pretty quickly.
HTML SLD not supported.
So let's go ahead and see what's going on there
in our main file.
It is very possible
that our A coding assistant did miss this.
Yeah, so our assistant missed this.
The one model missed this.
That's okay.
We can open up an ad hoc Ader instance.
We can add main.
So we can just come in here,
copy this previous command,
another advantage of writing out your prompts
and inspect prompt is that they're always going to be there.
So we can just copy this
and then all paste this in
and say only be sure to add not overwrite.
Yeah, and we'll just fire that off.
And now let's see what we get here.
Perfect.
Let's open up main and see if I got added great.
So now we have this HTML SLD,
you know, as you're working through these patterns,
you can always, always, always fall back
on just opening up an A coding instance.
In fact, as you're starting to use these patterns,
you should expect to come in,
clean up a couple things,
and then run our HTML SLD function.
Our main is now picking that up.
Notice if we had an actual test
running the analyze transcript,
our execution command and our evaluator method
would have picked up on that error.
So just want to call that out.
Testing is an important mechanism
for execution, evaluation feedback,
but also whatever execution command you're running
should represent the full state
of your application as much as possible.
So that I'll put a great,
let's go ahead and open up this file.
Let's go into HTML mode here, HTML preview, fantastic.
And so you can see here,
we have a slider at the top.
We have a kind of, you know,
straightforward HTML file.
Okay, and if we scroll this,
you can see something really cool happening.
Our JavaScript is executing
and our table is getting filtered, right?
So really cool.
Obviously this happened with a single prompt,
no styling here.
So it's just, you know, raw HTML and JS.
Really cool to see this come through
with the director pattern and that worked.
Let's style this back.
Let's close this and let's go ahead
and take a quick look at how the inside
of the director pattern look.
Most importantly, I want to show you
how the execution command feeds results
into the evaluator.
And I want to show you what prompt
we're running for the evaluator model
because as I said in the beginning,
we're using an LLM as a judge.
This is what makes this an agentic workflow,
you know, also known as a prompt chain
or, you know, a type of chain of thought
and really an AI agent, right?
It's when you start combining LLM calls together
for a specific use case.
That's what really makes this an agentic workflow.
So let's go ahead and just take a look at that.
We can see this in the director pattern.
So if we just open up direct
and we can walk through,
so we start out with a default empty evaluation.
If we open up create new AI coding prompt,
you'll see something really interesting.
If iterations equals zero,
so if we are on our first run,
we just want to run our base prompt
and in the case of our director slider,
the base prompt is this entire spec prompt file, right?
So we're passing that in.
If iteration is zero, if it's not zero,
we take the evaluation from our evaluator method
and we run this prompt,
generate the next iteration of code
to achieve the user's desired result
based on their original instructions, okay?
So here we're building the prompt to say,
hey, here's what we wanted,
where we have the base input prompt.
Here's the output and here's the feedback, okay?
Here we're building up a new AI coding prompt
when we have an evaluation
and we'll only have an evaluation
if we needed to run more than once, right?
You can see we have the feedback, output, base prompt,
and then we're letting our assistant know,
hey, you have this many tries left, okay?
So that's that.
We then have our AI code.
We're taking that prompt that was just built
and we're passing it to AI code.
And AI code, of course,
is just a lightweight wrapper around the MVP of it all.
The most important piece of this
our AI coding assistant.
And you can see here we have our editable context,
we have our read only context,
a couple configuration variables,
our model, and of course, our prompt, all right?
So just a classic Aitor call.
This is the keystone of this pattern.
The fact that we can use a high quality
AI coding assistant that's programmable
is what makes this all possible.
Again, this is why I've chosen Aitor for this course.
It's foundational, it's open source.
It's free minus the model costs.
This gives you control of everything.
It's so controllable that you can write your own AI coding
workflows, your own AI coding tools,
and full-on AI coding assistants using Aitor.
You can see here we're taking it
in a really simple fashion and just 300 lines of code.
We're building a director pattern
which closes the loop on AI coding.
Okay, so that's the AI code method.
Let's continue here.
So Aitor will run and have the side effect
of updating the files in its context window.
Okay, we then run our execute method.
And if we hop into execute,
we can see here we are just running
whatever our execution command was.
And if we go to our director config file,
you can see we have UV run pi test.
We're running all of our tests
and we're disabling warnings, okay?
Really simple, really straightforward.
We're writing that to our log file
and then we're returning both the out and the errors
if any exists, okay?
That's our execute.
And then we have another keystone
of the director pattern.
We have the evaluator method.
Let's hop in here and dissect this.
The evaluator method is building our file.
So it's creating key value pairs
between the path to our files
and the actual text to our files.
And then we have the second most important part
next to our AI coding system.
We have our evaluation prompt.
Our evaluation prompt is what guides the success
or failure of our execution.
You can see how this prompt works
by looking at a couple things.
You can see that we return the evaluation result.
So this really dictates what needs to happen,
market success, market failed,
and then give feedback if failed.
You can see we have a checklist.
You can come in here.
You can update this.
You can make this anything you need to, right?
The point of these patterns, the point of these ideas,
the point of these principles is for you to take it
and make it match your use case,
your code basis and your workflows.
We have the users desired results.
So this is our entire base prompt.
And remember in the case of our
director slider configuration file,
this is going to be our entire
markdown spec prompt file, right?
So that's our prompt.
We're then passing in our editable only files,
read only files, our execution command.
So that this prompt call is aware
of what we were trying to achieve, right?
So we need both the execution command
and the execution output to make that happen.
Okay.
And then at the bottom here,
we're just doing some JSON string parsing stuff.
We want to make sure that it's 100%
of JSON parsable.
This is how we're communicating to the LLM
so that we make sure that it's returning JSON
and use all this fail.
And I think our second or third iteration.
And we have a try catch for that reason.
The catch falls back to a GPT-4O model
with structured outputs.
Don't worry about these details too much
if they don't make sense.
Basically, we're running a fallback prompt
to parse the result exactly if this fails.
Okay.
But if it doesn't fail,
we run this completion
and then we run a pedantic parsing method
to load our evaluation result from the prompt.
Okay.
And again, all this is doing is saying,
hey, here was the state of the application, right?
Here's the prompt,
here are files that we were editing and reading.
Here's the execution command.
Here's our execution output.
Now tell me, did we complete the goal?
Did our AI coding assistant do the job?
It needed to, right?
We take the evaluation.
We check success or failed, right?
So this is it right here.
This is it.
Successor failed.
And if it failed,
our evaluation is now set for the next loop.
So we have our evaluation here.
And it runs right in to the new prompt.
And just like we saw in the beginning,
create new AI coding prompt.
Takes the evaluation
and it takes the index and what we're running on.
And builds a new prompt for the AI coding assistant
based on what we need to accomplish next.
So this director pattern,
this agentic workflow,
this small team of two,
this package of code wrapping around LLMs.
This is a full-on AI agent
that solves the problem of closing the loop
on AI coding workflows.
Let's come up from this.
And let's discuss the implications,
capabilities and limitations of this pattern.
We're engineers, we have to face reality.
We have to be real about everything that we're building.
And the capabilities of all of our tools.
The director pattern is an advanced AI coding technique.
It's in lesson seven for a reason.
It builds on Adder's secret.
The fact that Adder is a programmable AI coding assistant.
The director pattern requires a deep understanding
of your code as well as the expertise needed to plan
out work in advance.
The spec documents,
the AI developer workflows,
none of this matters.
If you can't plan out a feature from end to end
or at least a good chunk of a feature.
I don't expect many engineers taking this course
to fully utilize the capabilities of the director pattern.
That is just my expectations.
I would love to be proved wrong.
I would love for you to take this,
roll it in to a couple of your code bases
and really get insane value out of this.
In lesson eight, you'll see additional use cases
of the director pattern really showing off its capabilities.
It really enables you to hand off the hard work
to your AI coding assistant.
And when you learn when to use the director pattern,
you'll find how powerful this can be.
Future AI coding tools will embed this pattern
or patterns like this.
What we're saying here is here's what I want done, right?
In a great detailed plan,
let's go ahead and open up the spec prompt again.
You know, there's a lot of detail here, right?
We're asking for a lot and we should be as models progress,
as tools progress.
AI coding assistants will be able to do more
with powerful models.
So we want to push our spec prompts to the limit.
A massive side effect of models improving
and tools like ADA existing is that we can plan
much more work than we think,
especially with next generation models,
specifically reasoning models.
Okay, so just want to call that out.
Using a more powerful model will let you do even more.
Using a one preview, for instance,
and the evaluation will catch even more things.
So what are the other implications of the director pattern?
Of a pattern where the code sees the output of itself?
This pattern hints at the future of AI coding.
It means that engineers that understand what they want
to accomplish from start to finish can hand off nearly
the entire process to a self directed AI coding assistant.
It means planning is the key to great engineering.
It means knowing how to evaluate what you can do
is another key to great engineering.
It means writing high quality AI coding prompts,
gathering the right context
and selecting the right models are foundational AI coding skills
as we know through the lessons in this course.
The implications of this pattern and the spec prompt and ADWs
means that you can now wield AI coding assistants
like never before.
It means you can close the loop and let the code write itself.
Take these principles, take these patterns,
take these techniques and AI code like never before.
In lesson eight, the final principled AI coding lesson
I'm sad to say we close the loop on this course.
But we go out with a really important impactful lesson
you'll see whenever you're ready for lesson eight.
So this pattern of course has limitations.
As you've noticed throughout the course,
the biggest limitation here is us
and I'm not trying to be gimmicky.
I really mean that.
This is not an end all be all solution.
This is an instance of a true working example
of an agentic AI coding assistant
that runs a simple yet powerful closed loop
work in generate code based on your big three.
Then it executes the code, evaluates the result
and then improve if needed.
The pattern is the value.
The pattern here is what matters.
If we open up the director,
there are many ways that this can go wrong.
There are many cases it doesn't cover.
But what it does here is absolutely serves
as a 80% starting point for using the director pattern
and building out a more comprehensive version
of the director pattern in your code bases.
As you know, every code base is different.
There's always a little tweak.
Something a little different.
This flag needs to be here.
That flag needs to be there.
You need to run this set of command first.
Yada yada yada.
The director pattern does need to be fine tune.
Definitely more than any pattern we've covered
because we're not just generating code now, right?
We are closing the loop.
We're running the execution command.
We're looking for output.
And then we're taking that output,
running it against a powerful model.
Reasing models will be the best at this.
And we're having the reasing model evaluate the result
given the state of the execution.
The tooling we're using is in a small isolated use case
of this pattern, right?
This is just a starting place.
You'll see a coding assistance adopt transform
and build on this pattern in different
but similar ways.
The largest limitation of this pattern
is that if your execution command
does not provide feedback output,
this pattern will not work.
So in addition to writing,
you're prompt planning out your big three.
The task is now on you to create solid feedback
for your AI coding assistant.
And this is a good tell
if you should be using the director pattern
on one of your tasks.
Can you create a closed feedback loop
for your AI coding assistant?
If you can, if you can run a CLI command like this
or if you can run some workflow
where you write some UI and then run a vision agent
on the result to evaluate the response
or if you can generate a file
that the execution command can then validate exists
and maybe look at the content of it,
then you should be using the director pattern, right?
This is a, it's a limitation
but it's also just the reality of engineering.
In a lot of ways what we're doing here is
we're giving the AI coding assistant
and we're giving our AI tooling
the ability to do what we would do as engineers.
We would go ahead and do something like this.
I would run this test, you know, UV run,
main and just run the application.
Think about how you can write that into the director pattern,
right? Let the code write itself
by giving it the right execution command
and then by building out the right evaluation method.
We saw here our evaluator prompt looks like this.
And of course the link for this code base
for all this is going to be in your loop box.
You have full visibility into what this looks like.
You can take this, you can tweak it.
You can evaluate the result in many, many different ways.
I'm confident there's a better prompt for this,
but this prompt gets us 80% of the results.
Again, just to summarize,
the largest limitation for the director pattern
is needing access to an execution command, right?
We need something to evaluate
the success of your AI coding prompt against.
And this is why usually if you're going for the director pattern,
you want to use it with a spec prompt, right?
With a huge set of changes
with an entire feature kind of dialed in
to the spec prompt detailed out
because then your AI coding assistant
using the director pattern
can have a concrete evaluation command
and then a great evaluator model
and a great evaluator to run and integrate on the results.
The biggest thing I want to highlight once again
great planning is great prompting.
You can see that we can now close the loop
and let the code write itself,
but it's not useful and it's not efficient.
If you don't write great plans
packed with information dense keyword prompts,
this is going to be a continuing trend
in the age of gender of AI.
Those who can write more plans,
those who can write clearly,
those who can translate their thoughts
into a LLM compatible format
like the spec prompt will receive asymmetric results.
I hope this principle is clear to you.
In the age of gender to AI,
great planning is great prompting
and great prompting enables you to attain
asymmetric results before anyone else.
As you'll see in the next lesson,
the things I can do with these AI coding techniques,
they're not rivaled right now.
I'm just going to be point blank.
This is not me being cocky.
It's not me trying to put any other engineer down.
There's just not rivaled.
I don't see it anywhere.
And trust me, I've looked.
I don't see the patterns I'm using anywhere
in a meaningful capacity.
And trust me, I have looked.
So this is true for all AI coding tools
and truly for all AI tooling.
Next generation models and raising models in particular
will multiply your planning abilities with every release.
So get your reps in now.
The director pattern is the natural evolution
of the work we've been doing.
Spec prompts, great prompting,
clear prompting, managing your big three,
scripting with your AI coding assistant,
writing larger code changes in a spec prompt,
writing out AI developer workflows
that completely automate repeat work.
And now you can close the loop
and let the code write itself.
Congratulations.
You have made it to the end of lesson seven.
This is a large lesson to chew.
If you're really thinking through the implications
and the capabilities of the director pattern.
Yes, it requires more upfront investment
and requires you to think through
how you will execute and evaluate your code
on a per code base per feature basis.
But as you'll see in our final lesson,
when you put this all together with every principle
and every technique we've discussed,
your AI coding productivity will be unrivaled.
Take these ideas, take these principles,
put them into practice,
and multiply your productivity far into the sky.
If you're just watching,
I highly recommend run the director pattern
so you can feel the results.
Clone this code base, run the second director
config we were looking at, understand the spec prompt,
understand ADWs, AI developer workflows,
and understand that the director pattern
are all a reality for you now.
These are techniques you can use today right now.
The most important thing you can do is start.
Get your reps in, practice these patterns.
You want to feel your engineering shift from the how
to the what and internalize the fact
that great planning is now great prompting.
Lastly, if you're ready, close the loop.
Let the code write itself.
Take the director pattern, make it your own,
fine tune it, tweak it to fit into your code bases
that you're in on a daily basis.
Now you should be writing director configuration files
with prompts and then just executing that,
increasing the max iterations,
the more you see that you need them,
and the more that you can see your AI coding assistant,
indicating real work done inside the director pattern.
Give your AI coding assistant feedback
by running the command you would to understand
if you wrote the code properly.
Then create an evaluator that decides if the output is correct
or if feedback is needed.
Then structure feedback for your AI coding assistant
to loop on.
Once you see this, once you run the director pattern,
you'll see the future of engineering today.
In our last lesson, we stop.
We put together everything we've learned.
Great work here and I'll see you in the final
principled AI coding lesson.
