# how-to-suck-at-ai-coding-v4

**Source:** how-to-suck-at-ai-coding-v4.mp4

## Transcript

 Welcome to lesson 4 of Principled AI Coding. Now that you have a great grasp on AI coding, we're leading the fundamentals and entering the intermediate zone. In this lesson, we're going to look at how AI coding goes wrong so that you can resolve these common issues. As we've touched on, AI coding is about hitting the big three bulls-eye. Not just once, but throughout your AI coding sessions. That means when your AI coding tool has hallucinated or caused an issue, one of your three elements is likely off and needs to be addressed. This lesson is about fine-tuning your scope so that you can become an AI coding sniper and always hit the big three bulls-eye by avoiding common pitfalls that chew up your performance, time, and engineering output. After we discuss pitfalls and their solutions, we'll focus on boosting your capabilities over the long term by digging into new aspects of your AI coding assistant. Let's turn our attention to the principle for this lesson. Balance, then boost. First do it, then do it right, then make it fast. In lessons 1 through 3, we learned how to do it. In this lesson, we focus on doing it right. When you're coding with AI, you first want to align your big three bulls-eye by balancing your prompt, context, and model. Then it's all about boosting your capabilities by mastering your AI tooling and pushing it to its limits. Let's break down common pitfalls for AI coding as well as their solutions. These are issues you're going to see again and again, even with next-generation language models and great AI tools, but after this lesson, you'll know exactly how to fix them as soon as they happen, and you'll know how to prevent them from happening in the first place. To get started, let's go ahead and open up our terminal, and if we type LS, you can see our previous three lessons. Let's go ahead and clone in the lesson 4 repo. Link is going to be in your loop box. I'll type get clone, drop in the lesson 4 code base. If we type LS, you can see we have the fourth lesson there, so I'll go ahead and cd into this, then I'll open VS code here. Be sure to paste in your lesson3s.environment file here. Let's open our main file, and we'll go ahead and collapse so that we can see what we have here. From the file explorer and from our main file, you can see we have a slightly modified version of our lesson 3 code base. We're going to use it as a playground to showcase common pitfalls in AI coding. Let's open up the readme. Inside of this file, you'll have the set of instructions, and you'll also see a concise list of the common pitfalls that we're going to work through today across the context, model, and the prompt. I'll go ahead and collapse these so we can focus on them one at a time. Let's start with the context. This is the first place things go wrong with AI coding. There are two main varieties of context pitfalls. You're either missing context, or you have too much context. Let's start with missing context. Missing context is one of the easiest and one of the most common AI coding pitfalls. Let's run this setup command, and then we'll execute this prompt to see this in action. I'll open up a terminal, and let's copy the setup command, paste this in the terminal. This is a new syntax that you can use to start AIDER with files in the context window. So we hit enter here. AIDER will automatically boot up with these files in our context window. You can see those two here. Then we review our code here. We can pull in the main. We can also command click to open our main, and command click to open our output format right from the AIDER chat window. Let's go ahead and set up our prompt, add a new output format, Tommel. We have a simple high-level prompt. As we saw in lesson three, we are missing a file. What file is missing? If we open up our file explorer, we can see that our parse is missing. If you remember, we have this parse argument function that is pulled in from our arg parse file in our context window here. That file is completely missing. We're going to be executing this. We are missing our arg parse file, and this is one of the most common issues that happens. You're just missing context. So you run our prompt, and some details get missed. If we just look at the AIDER output, we can see exactly what happened. We did successfully add the format function in our main file. We added the import properly. We got the output format here, and we added Tommel, which will help us parse Tommel, but we did not update our parse. This is a very simple, very straightforward issue. We'll skip running this command. As we know, the solution here is very simple. Add the files. This is not ultra-useful advice. The principled way to solve this problem across time is to always think from your AID coding assistance perspective. Always know what your AID coding tool can see. Here is a great practice. Right before you send your prompt, think, would I be able to solve this problem with these files and this prompt? This small practice can save you from the missing context pitfall. AID coding tools will progress, and they will improve their ability to pick up on missing context. Obviously, we're just working with three files on a small code base. So it's not a huge issue. But missing context files becomes a big issue when you miss one, two, or five plus files that need to be edited in a large context prompt. When you're running a prompt against 10 plus files, having the updates misaligned after your prompt runs can be time-expensive to fix. Every mistake we make costs something. For engineering work, it's usually time and resources. The cost of this mistake is low to medium-civerity based on how many files you've missed. We can use AIDer's slash undo command to revert the previous change that was just made. So you can see there we've just undone our tommel change, and now we can slash add our hard parse file. So this is the normal way to fix this. But I want to show you a great feature in AIDer and another reason to use low-level prompts. Here we'll automatically suggest adding a file if we explicitly mention it in the prompt. Let's clear out the chat so that we know AIDer is not cheating with the file history. And then let's run this update source. And then we'll just look for that hard parse file. And then we'll say comma start up PY. So this is just us adding a little bit more detail. We're saying make sure that you also update every other file. And then we'll say add a new output format tommel. And just to be super clear and blunt about it, you can see we did remove all of our previous tommel code. That's no longer an output format. Now let's go ahead and run this prompt with a reference to the file we want to add in the prompts. We'll hit enter. And you can see right away AIDer is asking us, hey, you mentioned this file, but it's not in the chat. Do you want to add it? Of course, yes. And now before this prompt executes, AIDer is going to add it to the context window. So now you can see that change coming through across one, two, three files. You can see that change added here in our argoparse. It imported the file. We got the output format change and we have the updates in the main file. We have a little linting issue. We can hit yes and AIDer will automatically fix that for us. As AID coding tools progress, context management will improve. But fundamentally, there will always be some process where we tell the assistant what we need changed or some mechanism where it guesses. And then we confirm not thinking about what your AID coding assistant can see is an easy mistake to make and among the most common. We'll go ahead and clear and let's go ahead and undo our change since we're just using this code base as a way to showcase common pitfalls and solutions. So go ahead and undo and let's go ahead and run it again to fully remove the tommel change. Now let's go ahead and look at the inverse issue, excessive context. So let's clear, hop back to our readme and let's look at the setup for excess context. The larger the context window, the more your coding assistant and the underlying large language model has to make decisions about where things need to go. Your low level prompts with locations, actions and details reduce the chances that this will happen. In large code bases with similarly named variables, functions, classes, files and directory names, it's just as confusing for a large language model as it is for engineers. Even great models will make mistakes when working with code that is hard to distinguish. Even great engineers make mistakes in an overloaded context situation where there are too many files that play. An overloaded context is essentially too much information at once. You can imagine the model becomes overwhelmed and confused, causing it to predict a bad next token and therefore cause issues. As you know, with even just one line off or one syntax error, your entire program is cooked. It's incredible when you think about it, how precise we as engineers need to be on a daily basis. Thankfully, these AI tools can help us be more precise and faster than ever if we know how to use them properly. To test this, we can run this massive Aitor startup command and we can paste this end here. What this will do is use these two glob patterns to add multiple files to your code base at once. If we open up our file explorer, this first glob pattern will add everything in our root directory and the second glob pattern will recursively add every Python file from every directory. We can kick this off and now you can see we have a ton of files in our context window. It's important to note that glob patterns and Aitor file adding in general will only add files that are not ignored via your dot get ignore file. So this is really important. It's only adding files that it can see through Git. So if we scroll up, we'll see something kind of interesting and important warning. It's best to only add files that need changes to the chat. Aitor is, you know, explicitly calling this out already. Your context is going to get overloaded. Your language model is going to have issues. So right away, we're getting a warning for that. I like that that's built right into Aitor. Even Aitor gave us a warning that we are walking into dark territory. Things are going to go wrong with this much information in our context window. We can type slash tokens to see where the distribution of all of our tokens are coming from across our context window. You can see here, there are some obviously large offenders. We have our UV lock file, costing about eight cents a prompt. A bunch of things that don't need to be in here. This is just an example of sloppy context management. Let's go ahead and run our prompt against this context management and let's see what happens. So we have this prompt update the analyze transcripts, V2 function to include additional logging and sentiment analysis and the next type version. So there's a couple things going on here. You can see this also isn't the best prompt. We'll talk about prompts more in just a moment. Let's go ahead and paste this in. Let me just show you a little bit of what we have here. So if I just click on our transcript analysis, you can see here I have a couple additional transcript related messages. These are kind of just mock methods, but you'll notice that we have a lot of similarly named methods here. We have Analyst Transcript. We have Analyst Transcript with Logging Analyst Transcript V2. Then if we open up our data types, you can see we have a couple of types that are named in a very similar way. In large production code bases, you're almost guaranteed to get code and files that are named nearly exactly with some small variations. Like here we have V2, we have Vnext. You can imagine there are subtypes of transcript analysis and maybe we have some enums or some other types. This happens especially when you're centralizing around a specific domain problem. So let's go ahead and just run this prompt and let's see what happens. So we'll go ahead and just kick this off and we have a huge context window here. There's a lot going on. We have a couple methods here that are going to replicate us being in a large production code base. You can see our A coding assistant is making its best guess at what to do here. It's trying to match against this V2 analysis function. It's adding some logging. It's returning this Vnext type. Frankly all in all, it's doing a pretty good job at just kind of guessing at which one of these methods we actually wanted and which one of these types we were looking for. Also important to call out, we just torched 15 cents by overloading our context window. So not only is a large context window confusing for the LLM, we force it to guess, we force it to make decisions. We're also just torching cash. We don't have to look through the changes. I'm going to assume that the LLM made its best guess based on the prompt and the context window. All things considered our A coding assistant is doing a good job here. If we hop back to our REME, it's important to note that a lower level prompt can compensate for you having an overloaded context window. There's always this idea of intersecting your big three elements. You can compensate when you're not managing your context properly. You can write a great accurate low level A coding prompt and it will shift through all the noise that you've provided it. But the long term principle based solution here is the same as having missing context. Always think from your AICoding assistance perspective, could you solve the issue with the prompt and the context you're ready to launch? If not, change it. In the case of an overloaded context, it's a medium severity issue because if your AICoding assistant guesses wrong and starts updating the wrong files, you now have to update those files as well as make the change that you actually wanted. Now let's talk about common issues with the AIC coding prompt. Common pitfalls with a prompt come down to two issues. Your prompt is either too high level or it's too low level. Let's focus on the first case because it's the most common. Let's clear out our chat window here and boot up AIDER with only our Python files. You can see we're using the glob pattern and we're being better about our context window. We're only adding Python files now so there will all be code relevant and now we have all these items in our chat. And then we have this prompt. Enhanced the visualization of our data top and bottom. So this is one of the most common issues and effect a high level prompt as we touched on is a prompt that is focused much more on the what and less focus on the how. So our two high level are over focus on what to do void of detail on how to do it. Although the tides are and will continue to shift toward the what and high level prompts, we still need to include enough detail so that our AIC coding assistant and the underlying language model know how to get the job done. There's a sweet spot. You want to be aiming for mid level prompts. There is a mid level prompt that we want to strive toward with every AIC coding prompt that we write. Sometimes the mid level is closer to the high level. Sometimes it's closer to the low level with more details. This is how prompt phrase structures we discussed in lesson three can be really useful. The location, the action and the detail really help drive accurate consistent results from your large language model. Let's look at this prompt that is too high level and does not have enough detail to get the job done right. Enhanced the visualization of our data top and bottom. So let's close a few files here and let's look at our main file. This version of the application has three visualizations. We now have a bar chart, pie chart and a line chart. If we open up our chart.py, you can see these methods here. Simple enough. We actually have three visualizations now. We were to run a prompt like this with your experience already from the first three lessons. You can already see massive issues with this. Enhanced the visualization. That's vague. What visualization of our data? Data is just a terrible, terrible word in general. There's very little information here. This is one of the words that should just be banned from engineering unless it's in some type of context. And then we say top and bottom. So what is top and bottom? Maybe the highest count, the lowest count. Our language model has to guess here. It's making many guesses about what we actually want to do here. Let's go out and just fire this off. And let's see what our AI coding assistant comes up with here. Let's see what the LLM comes up with. We'll just fire this off here. Language models are progressing. They're improving a lot. So it's going to come up with something here. But right away, you can see we're updating all of our visualizations. So it's firing off on all three prompts. We didn't say what. So it's tweaking some of the labels. It's adding some text to some of our points. It's doing a little bit of work on the title of the pie chart. And it's adding some annotations for our chart.py. So it's just too high level. What are we actually asking for here? Again, thinking from our AI coding assistant's perspective, enhance the visualization. Like, imagine if a product manager asked you to do this. What does this mean? Enhance the visualization of our data, top and bottom. So I chose and wrote a particularly big and bad high level prompt. But you get the idea. And I can imagine you have run prompts like this yourself. Maybe just testing something. Maybe you're just doing things at Hawk. But you can see the point. This is a prompt that's too high level is void of useful information that helps drive results. So we can go ahead and just kick off our application. And we can actually just see what our language model changed here. So if I just copy this command here, this go and run our transcript application with men count 15. We'll have three visualizations pop up here, first our bar, then our pie, and then our final chart. So we can see we have some, you know, a little bit of additional information here. It didn't get what we wanted. You know, we can look through the rest of our items here. We have this nice virtual currency pie chart and then a line trend. OK. So let's improve this prompt with IDKs. And let's make it rich in location, action, and detail prompt phrases. A pattern you can consider for writing concise, a coding prompts, especially the larger ones. I recommend you just open a file, open up the context, where I just have your terminal open here. So you can see the context and then just work on an empty file. So we can start by saying update chart up you while. And let me go ahead and run slash undo to revert the changes here. So we're back at our original state. No changes to our visualizations. And we can go ahead and run clear as well. Word count. Bar chart. OK. So here we're using a function information, dance keyword. And then I'll just go ahead and add the nesting here and the statement. So I'll say update the top quartile of data to be green comma. So we have a list of changes, a list of details that we want. Then I'll say the bottom quartile red, the remaining blue. You can see here this prompt is much more detailed without even adding that many more tokens, words, characters. Let's go ahead and just copy this and let's just look at these side by side. Look at how much information this has. We are now telling our LLM, hey, we only need updates to this one file. Regardless of how many files are in your context window, update this one file, update this function. And here are the changes I want you to make. Top quartile of the data, green, bottom, red, remaining blue. And of course, the language model knows what a quartile is. We have two information, done keywords here, rebuusing our clear concise update keyword here. And then we have our file keyword and our function keyword. So there's no confusion here, right? I can buy this prompt and I know exactly, I don't know the exact lines, right? I don't know how the LLM will do it, but I know what it's going to do with zero confusion. Okay. So you can see there it's creating the quartiles, it's worked the change, it updated only that function and only the chart file. And we can go ahead and just kick this off again, drop our, I mean, count threshold, we'll switch that to 10. And now before even seeing the output, I know what the output here is going to be, right? We have the top quartile, red, the bottom quartile, green, and the remaining blue. Just very simple, very straightforward. We can be very confident in our changes. Why? Because we left no room for confusion. We wrote a low-level prompt, rich in information, dense keywords and prompt phrases. When you follow structure, when you follow patterns, you get repeatable results. That's a lot of what we're doing in this course. We're trying to win not just today, not on this one prompt, not with this one tool. We're aiming to set ourselves up with principle-based approaches and patterns that we can reuse across a coding tools across time. In terms of the severity and the cost of writing a high-level prompt like this, definitely can be fairly large. I would put this at mid-severity, maybe mid-to-heavy severity. It's almost guaranteed that you'll need to follow up and write at least one additional coding prompt with more detail. Why not just skip all that in your first prompt, right? Just write a great low-level prompt to begin with. Low-level prompts enable you to write perfect prompts for your egg coding assistant. It's going to be able to digest and then start working for you with no issues. If it does, by chance, hallucinate. Let's say our assistant hallucinated with this somehow. There's really not a lot you can do, right? You've pushed it to the limit. You've given it everything it needs. You've done your job just like working with a coworker. You can't know what they're going to do with 100% certainty. You can get close, right? As you build up that trust, just like we're building up our trust with these large language models and with our AI coding assistant. Low-level prompts is a great habit that prevents issues from arriving when your AI coding in real production applications with hundreds and thousands of files. With a large 10-plus file context window and with real requirements, you want to use low-level prompts to be precise and to avoid spending more time writing more prompts. Now, let's go ahead and look at the inverse case of this. Let's take this exact same prompt and hop back to our readme. Let me just copy this out. Let's copy this. Look at the readme and the prompt is too low-level. We're going to do the same thing here. Let's go ahead and we're worth this change. Show again, just slash undo. We can clear, do a full reset here. These are basically the same files. It's not a huge deal. Let's go ahead and take this prompt and solve the opposite issue. The prompt is too low-level. I'll admit, this is a super low-cost pitfall and few engineers, few AI coders are actually going to be making this issue of writing a prompt that's too detailed. Some of our super detailed oriented, maybe that's you, maybe you fall into this camp. Let me change the file load here. But very few of us are going to be writing super high-quality prompts and wasting too much time. This is the end of the spectrum you want to be on, especially when you're starting and especially as you're pushing your AI coding tool to do more for you. Low-level prompts are a great place to start and they're a great place to return to as you find yourself writing higher-level prompts, which are by design void of detail. At some point you're going to make a mistake, writing a high-level prompt moving fast, that's bound to happen. When it does, move to a low-level prompt. When you have a low-level prompt like this, we can resolve this by practicing and just by being more concise with the prompt. Let's try to rewrite this prompt to hit the sweet spot. This prompt is in fact a little too low-level. We can condense it even further. Let's try to get to that mid-level. We can rewrite this prompt by saying the following update. Let me just quickly turn and co-pile it off. Word, count bar chart, make, top quartile green, bottom red, rest blue. Same thing. Let's do a token check. This is 16 tokens and above we have 33 tokens. So nearly a 2X improvement in total tokens, roughly total time to write. We can go ahead, kick this off just as before, make sure that everything's all clear, no history. We can kick this off. This mid-level prompt is going to do basically the exact same thing. It got the exact same work done. We can see it right here, search or place. Another shout out to Adder as an AI coding tool. It's very visual. It shows you what it's doing. The observability is fantastic. But you can see here, we got that replacement just as we asked. We can go ahead once again, hit up. We can kick this off again and same result. This is the important thing to call out with AI coding tools. We have the same results in half the prompt. We cut the prompt in half. We got the same results. How? Why? Information, dense keywords, like update and make to steer the actions of our prompt phrase. Then we use the function information, tense keyword to specify a location. This is very important. Within our context, there is only one word count bar chart. We can see that and our AI coding assistant can see that. There is no confusion about where this update should go. Our AI coding assistant either has to find the spot or it has to blatantly hallucinate. That's exactly what you want. You don't want any fault to fall on your prompt. We added our details. Our simple list details. Again, lots of information here. Packed in, we're using the list and we're allowing our LLM to infer the pattern we want. Top core tile green, bottom, core tile red. That's hidden information here. This is embedded information, reason, comma, so it's a continuation. And the rest blue. This prompt is nearly perfect. We might be pushing a little bit to the high level here. You could imagine the core tile information may be getting missed using this rest pattern, but unlikely, and these are the prompts that I recommend you try to aim for. This is kind of a gold standard. But the cost of writing a prompt that's too low level like this is just much lower, right? Because what's the worst thing that happens? You spend a little bit of extra time, say, say a minute, writing a great prompt phrase information, keyword, dense prompt that does the job perfectly. And, you know, that's pretty great. Out of all the common pitfalls, this is the least offensive one. Not much more to say here. Let's move on to common pitfalls surrounding the model. So language model issues and pitfalls are simple to see and simple to solve. The biggest pitfall is usually using a weak and cheap model. This is a huge mistake. I see a lot of beginners make and the serious cash savers make. And we can easily showcase this. Let's go ahead and run slash undo. Let's actually just clear out this Adder instance and let's run this command. Another cool feature in Adder, you can run Adder from the command line and kick it off with a specific model. So of course, we're going to use DBT40 many as our weak cheap model. It's a fantastic model. But when you're writing code, when you're trying to save time, this is not the model to choose. Let me show you why. So we're going to add all of our Python files once again. And then we're going to run this prompt that we did before. Right, add a new output format, Tomal. Right, and remember, we reverted our Tomal change. So if I search all throughout the code base, you can see it's only here in the readme. Close this and let's just kick this off. Right. Let's see what this weak model does. Right. So we'll just go ahead and kick this off. It has all the context it needs to solve this problem. But it doesn't have the juice. Right. It doesn't have the intelligence to even in this small code base. Look at everything, digest the context that just the prompt and actually make the change we need. You can see it missed our arg parse file. And it's just kind of hard to miss. Right. This is a hard thing to miss. We have our parse arguments function here right at the top, which contains our output format that we kind of need. Right. So our small model, miss this, you know, you might be thinking, can't you write a lower level prompt to give GPT 40 many more guidance on how to solve the problem? Yes, you absolutely can. Low level prompts and context trimming will give for oh many a better chance. But weak models have limits. The only question is where is it when you use a higher tier model like Sonic 3.5, it will be immediately clear that using cheap models force you to do more work managing your context, writing lower level prompts, which will ultimately burn up more of your time. This does not scale well into larger code bases. You saw it here with only what 11 files in our context. It made a mistake with just 11 files. I completely understand the desire to save money, not overspend and to use the cheapest tool that can get the job done. This is smart thinking where it stops being smart thinking is when you don't include the value of your time and the experience you'll gain by using the best model at the right price. Forgive me if I sound a little harsh here, but if you fall into the cheapest model, shortest prompt, smallest context, in an effort to minimize costs, you're not thinking long-term enough and you're not getting your reps in for when the models inevitably do become cheaper and you're able to use powerful models to get a lot done. You won't know how to utilize them properly. Every ad coding prompt you run improves your experience with ad coding tools. Every ad coding prompt you run saves you time because you're offloading the code generation to your coding assistant and the LLM. These models will continue to get cheaper and they will continue to improve. Right now it's vital that you focus on getting things done and understanding the capabilities of great base models. Don't waste your time writing super low level prompts and paying too much attention to trimming your context window at the cost of going cheap. Right? When you're using Sonnet or GPT-40 or a really solid near state of the art language model, that's when you want to trim your context. That's when you want to use proper low mid-level prompts, but you don't want to do that because you're trying to cheap out on this stuff. There are hundreds of situations where I could have saved money doing things a cheap way, but the loss would be tremendous. We can never get our time back. This is the most valuable resource. This lesson and this course is meant to help you save time so don't cripple yourself by going super cheap when you have the option to use a legit powerful model. I would not have been able to push my abilities if I was using cheap models and I would not have been able to create this course or get even a quarter of all the work done that I have with these incredible tools. Great engineering is about making trade-offs and prioritizing speed cost performance. Pick two. I highly recommend you optimize for speed and performance in the generative AI age with your AI coding assistance. Every prompt you write is an investment in your time. This is what great technology does. You invest in it and it gives you back more value while consuming less every time. Let's move on. The inverse of this, of course, is model overkill. As an exact example, we can undo these changes. Go ahead and just clear. Re-runator. Here you can see we're booting up with one of the state-of-the-art models at the time of filming. This model for many of the problems, frankly, nearly every problem that you could throw at this small code base is likely overkill. We can paste this prompt in using our powerful O1 reasoning model. We can run this. It's basically that same mid-level prompt update word count star. This will update all of our word count visualizations, so not just the bar chart, but all three. We're using this kind of globs and text pattern that language models know really well. So it knows that we want to update all three of these. We can hit enter. Really, there's nothing wrong with this. The severity of running a prompt like this is low to mid. But as you can see, this is taking some time. I'll likely have to cut this part out of the video. I don't want to just waste time. There you go. The most obvious thing is just to not overkill. This is a really simple one. It's unlikely you're going to make this mistake, but it's important to call out because it finishes the picture on balancing the big three bulls eye. You don't want too much or too little of anything. You don't want to overkill with a very powerful model, like a reasoning model. When you're doing small problems that don't need the power, they don't need the speed, and frankly, they don't need you to cook your wallet like this prompt just did for me. So in the next lesson, we're going to be using these powerful reasoning models the right way, really pushing the amount of code and the amount of reasoning and the amount of problem solving that they provide us. We're going to be focusing on that in our next lesson. So stay tuned for that. And I hope you can see the big theme of every one of these common pitfalls we've discussed, right? With the prompt, with the context, and with the model, you'd see there are too little or too much, right? Too little, too much, too little, too much. This is why this is a balancing game. Our principle here is balance, then boost. OK, so we've covered many pitfalls. You're likely to run into while AI coding at scale, at an intermediate level. Most AI coding in this range in the zone is about avoiding too little and too much across your context model and prompt or balancing the intersection of the big three. Now after you've balanced your context model and prompt, you can start boosting your AI coding by going bigger. As we've mentioned in our next lesson, we're going to push into using reasoning models and an agentic pattern for dual model coding. Adder has an entire mode around this called architect. We'll get into that in the next lesson. But even before we get there, there are many things we can do to boost our productivity with AI coding tools. I want to laser focus in on Adder as a base level AI coding tool and break down how you can get more out of it. It can be easy to start using tools like Adder, get a ton of value out of the core features and miss out on going deeper. I chose Adder for this course for many reasons. Open source, terminal base, unbiased control, great resource behind it, great experimentation behind it, and also due to the fact that it's customizable to the core. Digging into your ad tooling beyond the surface will help you get everything you can out of your tool. Since using Adder and since it's foundational AI coding software, let's look through a few lesson known features this tool has that can give you a great idea of what a comprehensive AI coding tool can offer. Remember this course and AI coding is not about a single tool. AI coding is a skill that you can transfer to any tool, especially with the guiding principles we're covering in this lesson and in this course. Adder is the best tool to start with because it is a foundational tool that's simple, open source, yet highly configurable. It's a tool that current and future AI coding will intersect mirror and straight up copy. It's funny to think most engineers won't know that Adder was one of if the not first tool to have multi-file editing. Guess who everyone looked to to understand how to build this feature? Adder. Everything seems so obvious after it's been done right. I hope this course is starting to feel that way for you. This natural focusing on the context model prompt using powerful information rich keywords in your prompts, keeping things simple, starting with more detail, moving to less, it should all feel obvious once you hear it and use it. If it does, that means I'm doing my job as your instructor here. Let's talk about valuable features in Adder and in other tools that can boost what you can do with AI coding. So if we open our file explorer here, you'll notice this dot Adder yaml example file. So let me close main and let's open that up. I'll change the name so that it has this proper extension and this is what an Adder configuration yaml file looks like. This is a reusable configuration file that Adder will read from either from the root of your code base or from your home directory to automatically set up your preferred configuration when you type Adder. There are many settings and flags here. Let's walk through a few of the highlights. First off, you can set up your default model. So let's say you wanted to always start with the O1 mini model. You can comment model in and then type Adder. And now when it boots up, since it's in the root of this directory, you can see we're automatically starting the application with O1 mini with the best configuration. So this is great for model reuse. We can comment that out. And if you just scroll up here, you can see we have the default open AI API key. We also have the setting for the anthropic API key. We can comment those out here. Another great feature in Adder is that you can set up default file. So if we search for file colon, you can see we have this flag. We can comment this in. Let me just go ahead and close that. And now we can just type a file. So let's say we wanted to always have our readme. So this will now always activate. Let's go ahead and close Adder and restart. If we start Adder now, you can see that we automatically have readme added to our context window. So this is a really powerful mechanism for always starting Adder with specific files. You can imagine you have documentation, style guides, patterns, you know, outside resources that you imported that you always want to import as a guide on startup in Adder. So this is a really powerful feature. Another great way to use this feature is to not add it to the file flag, but to actually add it to the read flag. This makes Adder start up with readme as a read only file. Let's go ahead and open up Adder again. And you can see we have this file added, but it is a read only file. This is really important. If you're adding files like documentation, syntax guides, you know, AI coding guides, this is a common convention, you can set that all up in Adder on boot up every single time by adding read only files in the Adder configuration file. This is a really powerful pattern. Now when we write prompts, we will not have the option to update this. This is a great way to push your context management further and let Adder know, let the LLM know what files you actually want to be changing versus strictly reading from. We have the suggest shell command. I like to disable this throughout my code basis. It's useful when you're starting out on projects. I have Adder suggest things, but I like to disable this so that I run the commands that I want to run. There is this auto test flag and this test command. This makes Adder run tests after every single prompt you execute. So when you start moving fast with AI coding, you can enable this and say you want to set pi test true. After every prompt, Adder is going to fire off pi test, run the test, add it to its context window optionally, and then based on the results of the test, if there's an error, it will automatically try to fix the error. This is a great way to get more out of your AI coding assistant. It's a great way to move fast and have confidence in the work that you and your AI coding assistant are doing. A couple of mentionables. You can use Adder in the browser by enabling GUI mode here. You can use Adder's voice mode as well. Links for all this stuff, links for Adder's configuration file documentation. It will all be below in your loot box. Using a reusable configuration means less setup work and more reusability across multiple code bases. What you're seeing more and more while Adder is a foundational AI coding tool. It's made so that you can configure it, customize it, and use it across many code bases with hundreds and thousands of files. I recommend you set up an Adder configuration file in your home directory and on a per project basis. In addition to using file configuration, Adder has a decent list of commands we have not covered yet. So let's hop back into Adder here. So as you saw in our readme, we selected the mini model at some point here. We can change and list all the models that Adder has available to us with these slash models command. So if we type slash models, and let's say we're looking for all the Sonnet models, we can type Sonnet, and Adder will give us all the Sonnet models available to it. It's important for this command that you specify some text here. Otherwise it won't do a search. So if we type slash models and we look for a one, you can see all the variants of the O1 model available to us at this time. If we type slash model, we can of course change our model. Let's say we want to go to the O1 preview, which we're going to cover in our next video. You can just type slash model, and then you can automatically change in your Adder to that model. Okay. Slash read only to add read only files into the context window. We saw this with the configuration file. You can also do it using this command. If we type slash readme, if we type main. If you only wanted to have these files in read only mode, you can use and set up add. You can use slash read only. This is a really powerful pattern for keeping your context window clean. Let's go ahead and run reset to clear both the chat window and our file. So this runs slash clear and slash drop. You can use slash get to run any get command. So if you run branch, you can see that we have our main branch there. This is a quick way to an Adder just run arbitrary get commands. You can run slash settings. And this will print out all of your settings and your application state loaded from your configuration file and just all the defaults that are set in Adder and any updates that you make inside the Adder command. You can see we're running the GPT-40 model here. You can use slash run to run arbitrary bash commands and then have the output of your commands attach right back to the Adder window. So if we just type LS here, you can see that we're listing all the files on our current directory. Do you want to add the output to the chat? You can say yes. And now Adder will have that context in the chat window. So if we type slash tokens now, you can see we have a small chat history here with the information that we just ran above. So there are more commands. If you type slash, you can see a whole list of commands you can run here. It's important to note that all AI coding tools moving forward will have some version of these kind of foundational, low level, essential commands. How to recommend you experiment here? This is how you can boost your AI coding tool productivity. You experiment, you play with this. Try commands that you haven't before. As all important docs, this is going to be below in your loop box. I'll have links to Adder's documentation for you to check out. This lesson cannot be complete. If we didn't discuss how to keep up and how to focus on information that matters that enables you to evolve with the AI industry, right? With the AI tech industry, AI tools are evolving every day from big tech to individual developers. AI coding tools are being created and improved on a daily basis at a rapid pace. Engineering tools have always been different because they help us build more and better tools. AI coding tools are the ultimate version of this because as these tools improve, as Adder, cursor, Z, get up copilot, get up spark, all these variations of what is essentially UI, wrapped in some code, wrapped in some prompts. As these tools improve, we improve and then we improve the tools and it creates this recursive feedback loop of immense productivity. Adder is a great example of this. The creator of Adder uses Adder to build Adder. Same with the Devon engineer, same with the cursor engineers. Every engineer building these tools are getting meta level results out of it. That means in order to keep up, you must be using these tools and you must keep your eyes on the ecosystem. You've made it to this course and you've invested in yourself. This is a massive win for you that will likely change the course of your engineering career, especially with the final half of this course that you are bought to embark on. I'm really excited to share the next generation coding ideas we have in the remaining half of this course. You already know who I am. You already know what I do. If not up to date, I'll be covering the best coding patterns and principles on the YouTube channel. Link will be in your loop box. Another reason Adder was using this course is because the creator knows what he's talking about. He has benchmarks, experience and data to back up all of his work. I'm going to recommend the Adder leaderboard so that you understand the best model to use to get the best echoing results. This is updated regularly when new models are released. I also recommend any blog release on the Adder website as well. There are many resources and places to go. The key is that you look for reliable sources of information from real builders and real engineers. In the age of generative AI, in the age of generative AI, there will be endless junk and cheap derivative content. Avoid these and look for the sources of experience, proof of value, experiments and data to back up their claims. Now you know the most common ways you're likely to make mistakes with AI coding, with your AI coding assistant. But now you also know how to fix them. Balance, then boost. First, make sure your prompt, context and model are aligned. Always start by writing low-level prompts using IDKs, which are information dense keywords recovered in lesson three, and prompt phrases to write high quality prompts. As you progress, you can play with mid to high-level prompts to save time. But whenever you get issues with a high-level coding prompt or you missed details, always shift yourself back to the low-level. Focus on getting things done, not saving a couple keystrokes here and there. Then add only the context you need to solve the problem you're working on and think about the context from the perspective of your AI coding assistant. Or you run the prompt, think with this context and this prompt, would I be able to solve this problem? Imagine your product manager or a fellow engineer asks you to do something and they gave you this context and this prompt exactly. Would you be able to solve the problem? Is enhance the visualization of our data top to bottom enough for you to really solve the problem? Lastly, don't overuse reasoning models and don't go cheap on your base language model. Right now, GPT-40 and Sonic 3.5 are solid, high-performing base models. This will change in the future. Whatever aader has set up as the defaults, I highly recommend. In our next lesson, we're going to run powerful reasoning models to generate tons of code with a couple new techniques that will expand the size of the swing you can take with your AI coding tools. We'll show how planning is the key to accomplishing this. Planning is the key to shipping massive amounts of code. Congratulations on making it through lesson four. Give yourself a well-deserved break and I'll see you in lesson five where we'll multiply your AI coding abilities even further beyond.